{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf91845c-42e2-49f9-9bab-08ac5f283d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53acf401-477b-4fe8-a9cf-fc3b97cea28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Mental-Health-Twitter.csv')\n",
    "df2 = df[['post_text', 'label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968127c9-859a-45c5-8a6c-a1ca6682f6bf",
   "metadata": {},
   "source": [
    "## Part 1 - Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aa86ebd-992d-44d6-bc8c-1ec42aeeec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearing html items like &apos; ,&amp; ,&lt; etc\n",
    "import html\n",
    "\n",
    "df2.loc[:, 'post_text'] = df2['post_text'].apply(lambda x: html.unescape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "343dec3d-4390-4391-8e6e-288632f94df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding to utf-8\n",
    "df2.loc[:, 'post_text'] = df2['post_text'].apply(lambda x: x.encode('utf-8').decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a95ca0b1-76a5-4723-bb94-35044287682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing hyperlinks, hashtags or styles like retweet text\n",
    "import preprocessor as p\n",
    "\n",
    "def clean_tweet(text):\n",
    "    p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.HASHTAG)\n",
    "    text = p.clean(text)\n",
    "    return text\n",
    "\n",
    "df2.loc[:, 'post_text'] = df2['post_text'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a48c245-fd74-47e9-8e6d-7f61deafbf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace  apostrophes with the standard lexicons\n",
    "import contractions\n",
    "df2.loc[:, 'post_text'] = df2['post_text'].apply(lambda x: contractions.fix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb446580-3111-47f5-a23c-f3e60df01fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split attached words for eg - ForTheWin becomes For The Win so it makes sense\n",
    "import re\n",
    "from wordsegment import load, segment\n",
    "load()  # load word frequency data once\n",
    "\n",
    "def smart_word_split(text):\n",
    "    # Remove hashtags but keep content\n",
    "    text = re.sub(r'#([A-Za-z0-9_]+)', r'\\1', text)\n",
    "\n",
    "    # Split CamelCase or PascalCase words\n",
    "    text = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', text)\n",
    "\n",
    "    # Split letters and numbers apart\n",
    "    text = re.sub(r'(?<=[A-Za-z])(?=[0-9])', ' ', text)\n",
    "    text = re.sub(r'(?<=[0-9])(?=[A-Za-z])', ' ', text)\n",
    "\n",
    "    # For very long lowercase words (likely attached slang or glued text)\n",
    "    words = []\n",
    "    for token in text.split():\n",
    "        if token.islower() and len(token) > 12:  # heuristic threshold\n",
    "            segmented = segment(token)\n",
    "            if len(segmented) > 1:\n",
    "                token = ' '.join(segmented)\n",
    "        words.append(token)\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "df2.loc[:, 'post_text'] = df2['post_text'].apply(smart_word_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8620a3c8-7a3d-43d9-ad78-0b5f421a13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Convert text to lower case to avoid case sensitivity related issues\n",
    "df2.loc[:, 'post_text'] = df2['post_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b1c6d52-1468-4762-9f84-81e7046c0b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing slangs with meanings using a custom slang dictionary containing about 227 slang words/phrases\n",
    "with open(\"slang.txt\", \"r\") as file:\n",
    "    slang = file.read()\n",
    "\n",
    "# Separating each line present in the file\n",
    "slang_lines = slang.split('\\n')\n",
    "\n",
    "slang_word = []\n",
    "meaning = []\n",
    "\n",
    "# Store the slang words and meanings in different lists\n",
    "for line in slang_lines:\n",
    "    if line.strip():  # Skip empty lines\n",
    "        temp = line.split(\":\", 1)  # Split only on first ':' to handle meanings with ':'\n",
    "        if len(temp) == 2:\n",
    "            slang_word.append(temp[0].strip())\n",
    "            meaning.append(temp[1].strip())\n",
    "\n",
    "# Function to replace slang in a single text\n",
    "def replace_slang(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    tweet_tokens = text.split()\n",
    "    for i, word in enumerate(tweet_tokens):\n",
    "        # Remove common punctuation attached to words\n",
    "        clean_word = word.strip('.,!?;:\"()[]{}').lower()\n",
    "        if clean_word in slang_word:\n",
    "            idx = slang_word.index(clean_word)\n",
    "            # Preserve original capitalization and punctuation\n",
    "            replacement = meaning[idx]\n",
    "            if word[0].isupper():\n",
    "                replacement = replacement.capitalize()\n",
    "            # Reattach punctuation if it was at the end\n",
    "            if word[-1] in '.,!?;:\"':\n",
    "                replacement += word[-1]\n",
    "            tweet_tokens[i] = replacement\n",
    "    return \" \".join(tweet_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6927f513-a704-4907-bf4b-ad23fe5ccc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[:, 'post_text'] = df2['post_text'].apply(replace_slang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b62a1c7c-7cd6-42da-a210-8ff38e83eab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing and Spell Check\n",
    "import itertools\n",
    "from autocorrect import Speller\n",
    "\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "# Function to reduce repeated characters (no more than 2 in a row)\n",
    "def reduce_repeated_chars(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Group consecutive identical characters and keep only up to 2\n",
    "    result = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
    "    return result\n",
    "\n",
    "# Function to apply both: reduce repeats + spell check\n",
    "def standardize_and_spellcheck(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return text\n",
    "    # Step 1: Reduce repeated characters\n",
    "    text = reduce_repeated_chars(text)\n",
    "    # Step 2: Spell check (word by word to preserve structure)\n",
    "    words = text.split()\n",
    "    corrected_words = [spell(word) for word in words]\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "df2.loc[:, 'post_text'] = df2['post_text'].apply(standardize_and_spellcheck)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f81c825-ec11-4276-a184-bd7651d56e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopward removal using nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "stopwords_eng = set(stopwords.words('english')) \n",
    "\n",
    "def remove_stopwords(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return text\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords_eng]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "df2.loc[:, 'post_text'] = df2['post_text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0939d34b-28ea-4818-85c0-21da3d5b5f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Punctuations using regex \n",
    "df2.loc[:, 'post_text'] = df['post_text'].str.replace(r'[^\\w\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2a3a37-e85f-48b9-845d-52aadc832408",
   "metadata": {},
   "source": [
    "## Ensuring no duplicates were made by data cleaning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed5b8f09-24de-4d0b-987d-8073b639b5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique post_text entries: 19483\n"
     ]
    }
   ],
   "source": [
    "unique_texts = df2['post_text'].nunique()\n",
    "print(f\"Unique post_text entries: {unique_texts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc1272a7-66a3-4a8b-be32-376eeb9ccb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19483, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2.drop_duplicates(subset='post_text').reset_index(drop=True)\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f830ec37-7cb6-4389-9302-963fdddc6f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('cleaned.csv', index=False);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
